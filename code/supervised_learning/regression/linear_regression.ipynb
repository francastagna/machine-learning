{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a supervised learning algorithm used for predictive modeling. It is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. The goal of linear regression is to find the line of best fit that minimizes the sum of squared differences between the observed values and the predicted values of the dependent variable.\n",
    "\n",
    "The line of best fit is represented by a linear equation of the form: `y = b0 + b1 * x`, where `b0` is the intercept and `b1` is the coefficient for the independent variable `x`. The goal is to estimate the values of `b0` and `b1` that minimize the sum of squared differences between the observed and predicted values of `y`.\n",
    "\n",
    "Once the values of `b0` and `b1` are estimated, the linear regression model can be used to make predictions for new observations. Given a new value of `x`, the corresponding value of `y` can be predicted using the estimated equation.\n",
    "\n",
    "Linear regression can be used with one or more independent variables, known as simple linear regression and multiple linear regression, respectively. It assumes a linear relationship between the independent and dependent variables and can be used for continuous and categorical dependent variables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of performing Linear Regression\n",
    "\n",
    "There are multiple ways to perform linear regression, including:\n",
    "\n",
    "1. Ordinary Least Squares (OLS) method: This is a closed-form solution that uses normal equations to find the coefficients that minimize the sum of squared differences between the observed values and the predicted values. \n",
    "\n",
    "2. Gradient Descent: This is an iterative optimization technique that adjusts the coefficients in the direction of the steepest decrease in the cost function until the minimum is reached. Gradient Descent can be used with different types of optimization algorithms, such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.\n",
    "\n",
    "3. Ridge Regression: This is a regularized version of linear regression that adds a penalty term to the cost function to control the magnitude of the coefficients. This technique is used to prevent overfitting, which occurs when the model is too complex and fits the noise in the data.\n",
    "\n",
    "4. Lasso Regression: This is another regularized version of linear regression that adds a penalty term to the cost function to control the magnitude of the coefficients. The difference between Lasso and Ridge Regression is that Lasso tends to produce sparse models, where some of the coefficients are exactly equal to zero.\n",
    "\n",
    "Each of these techniques has its own strengths and weaknesses, and the choice of technique depends on the problem at hand and the characteristics of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (Or Minimal Squares)\n",
    "\n",
    "Ordinary Least Squares (OLS) is a method used to find the line of best fit in a linear regression problem. The objective of OLS is to minimize the sum of squared differences between the observed target values and the predicted values produced by the linear regression model. \n",
    "\n",
    "\n",
    "Ordinary Least Squares is a method used in linear regression to find the line of best fit that models the relationship between the independent variables (features) and the dependent variable (target). The goal of OLS is to minimize the difference between the predicted values and the actual values.\n",
    "\n",
    "The difference between the actual values and the predicted values is called the residuals. To measure how well the line of best fit models the data, the residuals are squared and summed. This sum of squared residuals is known as the residual sum of squares (RSS). OLS aims to find the line of best fit that minimizes the RSS.\n",
    "\n",
    "Mathematically, we can represent this difference as the Residual Sum of Squares (RSS), which is defined as:\n",
    "\n",
    "RSS = $\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "where n is the number of data samples, y_i is the observed target value for the i-th sample, and $\\hat{y}_i$ is the predicted value produced by the linear regression model. The goal of OLS is to find the coefficients of the linear regression model (w_1, w_2, ..., w_p) that minimize the RSS.\n",
    "\n",
    "![title](img/picture.png)\n",
    "\n",
    "![title](img/picture.png)\n",
    "\n",
    "![title](img/picture.png)\n",
    "\n",
    "![title](img/picture.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
